# ðŸ”§ MoCo v3 Fine-Tuning & Evaluation Suite

This directory contains scripts for fine-tuning a MoCo v3 Vision Transformer encoder using [LoRA (Low-Rank Adaptation)](https://arxiv.org/abs/2106.09685), as well as tools for various evaluation strategies.

## ðŸ§ª Fine-Tuning Scripts

These scripts perform LoRA-based fine-tuning of a frozen MoCo v3 encoder pretrained on car crops.

### ðŸ”¹ `lora_mocov3_current.py`

- Fine-tunes a classifier using LoRA on a **single specified year**.
- Designed for isolated year-by-year adaptation.

### ðŸ”¹ `lora_mocov3_incremental.py`

- Performs **sequential LoRA fine-tuning**, incrementally updating the model as new years of data are introduced.
- No replay; past data is not revisited.

### ðŸ”¹ `lora_mocov3_replay.py`

- Implements **LoRA with rehearsal (replay)** by training on data from all **previous years + current**.
- Mitigates forgetting in time-evolving data.

---

## ðŸ“Š Evaluation Scripts

These scripts evaluate fine-tuned models using various strategies.

### ðŸ”¹ `ncm.py`, `ncm_v2.py`, `ncm_time.py`, `ncm_time_current.py`, `ncm_time_replay.py`

- Evaluate using **Nearest Class Mean (NCM)** classifiers.

### ðŸ”¹ `fecam.py`

- Evaluates using **FeCAM**

### ðŸ”¹ `ranpac.py`

- Evaluates using **RANPAC**

### ðŸ”¹ `randumb.py`

- Evaluates using **Randumb**.


## ðŸ“¦ Requirements

Before running any scripts, make sure the following libraries are installed:

- [MoCo v3](https://github.com/facebookresearch/moco-v3)
- [Hugging Face PEFT](https://github.com/huggingface/peft)
- Standard Python libraries:
  - `torch`, `torchvision`
  - `timm`
  - `numpy`
  - `tqdm`
  - `tensorboard`