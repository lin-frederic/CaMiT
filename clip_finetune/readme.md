# ðŸ”§ CLIP Fine-Tuning & Evaluation Suite (LoRA-based)

This directory provides tools for **sequential fine-tuning** of a CLIP-based vision encoder using [LoRA (Low-Rank Adaptation)](https://arxiv.org/abs/2106.09685), along with scripts for evaluating model performance over time.

## ðŸ§ª Fine-Tuning Script

### ðŸ”¹ `lora_clip_incremental.py`

- Implements **sequential LoRA fine-tuning** of a frozen CLIP encoder using **[OpenCLIP](https://github.com/mlfoundations/open_clip)**.
- Fine-tunes only low-rank adapter layers **without replay** of previous data.

---

## ðŸ“Š Evaluation Scripts

These scripts evaluate fine-tuned models using various strategies.

### ðŸ”¹ `ncm.py`, `ncm_v2.py`, `ncm_time.py`, `ncm_time_current.py`, `ncm_time_replay.py`

- Evaluate using **Nearest Class Mean (NCM)** classifiers.

### ðŸ”¹ `fecam.py`

- Evaluates using **FeCAM**

### ðŸ”¹ `ranpac.py`

- Evaluates using **RANPAC**

### ðŸ”¹ `randumb.py`

- Evaluates using **Randumb**.

---

## ðŸ“¦ Requirements

Before running any scripts, make sure the following libraries are installed:

- [OpenCLIP](https://github.com/mlfoundations/open_clip)
- [Hugging Face PEFT](https://github.com/huggingface/peft)
- Standard Python libraries:
  - `torch`, `torchvision`
  - `numpy`, `tqdm`
  - `tensorboard`

